"""
Dynamic LLM-generated challenges.

When dynamic mode is enabled, challenges are generated by an LLM and then
verified by a second LLM call before being served. This produces novel,
creative challenges that can't be enumerated or cached.

⚠️ Dynamic mode adds 2 API requests to your hot path per challenge generation:
  1. One call to generate the challenge
  2. One call to verify/solve it

Supports: OpenAI, Anthropic, Google Gemini.
"""

import json
import logging
import os
import re
import time
from typing import Optional, Tuple
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError

logger = logging.getLogger("agentchallenge.dynamic")

# ── Provider configurations ───────────────────────────

PROVIDERS = {
    "openai": {
        "url": "https://api.openai.com/v1/chat/completions",
        "env_key": "OPENAI_API_KEY",
        "default_model": "gpt-4o-mini",
        "auth_header": lambda key: ("Authorization", f"Bearer {key}"),
        "build_body": lambda model, messages, temperature: json.dumps({
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 400,
        }),
        "extract": lambda resp: resp["choices"][0]["message"]["content"].strip(),
    },
    "anthropic": {
        "url": "https://api.anthropic.com/v1/messages",
        "env_key": "ANTHROPIC_API_KEY",
        "default_model": "claude-sonnet-4-20250514",
        "auth_header": lambda key: ("x-api-key", key),
        "extra_headers": {"anthropic-version": "2023-06-01"},
        "build_body": lambda model, messages, temperature: json.dumps({
            "model": model,
            "max_tokens": 400,
            "temperature": temperature,
            "messages": messages,
        }),
        "extract": lambda resp: resp["content"][0]["text"].strip(),
    },
    "google": {
        "env_key": "GOOGLE_API_KEY",
        "default_model": "gemini-2.0-flash",
        "build_url": lambda model, key: f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={key}",
        "build_body": lambda model, messages, temperature: json.dumps({
            "contents": [{"role": m["role"] if m["role"] != "assistant" else "model", "parts": [{"text": m["content"]}]} for m in messages],
            "generationConfig": {"temperature": temperature, "maxOutputTokens": 400},
        }),
        "extract": lambda resp: resp["candidates"][0]["content"]["parts"][0]["text"].strip(),
    },
}

# ── Prompt Engineering ────────────────────────────────
# Key techniques applied:
# 1. Constrained output categories — only allow challenge types proven to verify well
# 2. Explicit anti-patterns — list what NOT to generate
# 3. Many diverse few-shot examples covering each allowed category
# 4. Strict JSON output format with answer constraints
# 5. Answer must be a single number, single word, or very short deterministic string
# 6. Low temperature for verification (deterministic solving)
# 7. Aggressive answer normalization for comparison
# 8. Explicit "ANSWER:" prefix extraction from verifier

GENERATE_PROMPT = """You are a challenge generator for AI agent authentication. Generate ONE unique multi-step reasoning challenge.

RULES (strict):
1. The answer MUST be a single number (integer). Never a sentence, never a string.
2. The challenge MUST have exactly ONE correct answer — no ambiguity.
3. The challenge MUST require 2-4 chained mathematical or logical steps.
4. The challenge MUST be solvable by pure arithmetic/logic — no trivia or world knowledge.
5. End the prompt with "Reply with ONLY the answer, nothing else."
6. SHOW YOUR WORK: Solve it yourself step by step FIRST, then write the JSON.

CHALLENGE TYPES (pick one randomly each time):
- Nested arithmetic: ((A + B) × C) - D, then modulo E
- Base conversion chain: binary→decimal, apply arithmetic, modulo
- Sequence reasoning: given an explicit pattern, compute the Nth term
- Property chains: "Word X has N letters (TELL them N), multiply by M, subtract K"
- Modular arithmetic: compute expression, find remainder
- Digit operations: sum of digits of a number, or digital root
- Multi-step arithmetic: chain 3-4 operations (add, subtract, multiply, modulo)

FORBIDDEN:
- Character-by-character string manipulation (reversing strings, removing vowels, ROT13, Caesar cipher)
- Counting individual letters or characters in strings/phrases (NEVER ask "how many letters" or "count the letters")
- Asking the solver to determine the length of any word (always TELL them the length instead)
- Challenges requiring letter-level operations
- Single-step problems (must chain 2+ operations)
- Ambiguous geometric terms ("sides of a cube" — faces? edges?)
- Ambiguous answers or multiple valid solutions
- World knowledge or trivia questions
- Factorials above 6! (LLMs miscompute large factorials)
- Percentage/discount problems that produce non-integer answers (e.g. $55.08)
- The answer MUST be an exact integer, never a decimal
- Asking the solver to "count digits" in a binary or decimal number (models miscount)
- Any task where the solver must count individual characters, letters, or digits

EXAMPLES (DO NOT reuse — generate novel challenges with different numbers):
Working: (17 × 4) = 68, 68 + 23 = 91, 91 mod 13 = 0
{"prompt": "Compute 17 × 4, add 23, then find the remainder when divided by 13. Reply with ONLY the answer, nothing else.", "answer": "0"}

Working: binary 110110 = 54, 54 - 19 = 35, 35 mod 8 = 3
{"prompt": "Convert binary 110110 to decimal, subtract 19, then find the remainder when divided by 8. Reply with ONLY the answer, nothing else.", "answer": "3"}

Working: 5! = 120, 1+2+0=3, 3^2 = 9, 9-2 = 7
{"prompt": "Compute 5 factorial, find the sum of its digits, square that sum, and subtract 2. Reply with ONLY the answer, nothing else.", "answer": "7"}

Working: 3! = 6, 6 × 7 = 42, sum of digits of 42 = 4+2 = 6
{"prompt": "Compute 3 factorial, multiply by 7, then find the sum of the digits of the result. Reply with ONLY the answer, nothing else.", "answer": "6"}

IMPORTANT: Each challenge you generate MUST use a DIFFERENT structure than the examples above. Do NOT just change the numbers in a "compute X, add Y, modulo Z" template. Use genuinely different reasoning steps each time — mix base conversions, digit sums, factorials, word counting, sequence patterns, combinatorics, or property chains. Be creative.

Now generate ONE novel challenge. Different numbers, different structure. Show work first, JSON last:"""


VERIFY_PROMPT = """Solve this challenge step by step. Show your work, then write your final answer on the LAST line prefixed with "ANSWER: ".

Challenge: {prompt}

Work through it carefully:"""


# ── LLM caller ───────────────────────────────────────

def _call_llm(
    provider_name: str,
    api_key: str,
    messages: list,
    model: Optional[str] = None,
    temperature: float = 1.0,
    timeout: int = 15,
) -> str:
    """Make a raw HTTP call to an LLM provider. Returns the text response."""
    provider = PROVIDERS[provider_name]
    model = model or provider["default_model"]

    # Build URL
    if "build_url" in provider:
        url = provider["build_url"](model, api_key)
    else:
        url = provider["url"]

    # Build request
    body = provider["build_body"](model, messages, temperature).encode("utf-8")
    req = Request(url, data=body, method="POST")
    req.add_header("Content-Type", "application/json")

    # Auth
    if "auth_header" in provider:
        header_name, header_val = provider["auth_header"](api_key)
        req.add_header(header_name, header_val)

    # Extra headers
    for k, v in provider.get("extra_headers", {}).items():
        req.add_header(k, v)

    # Call
    try:
        with urlopen(req, timeout=timeout) as resp:
            data = json.loads(resp.read().decode("utf-8"))
            return provider["extract"](data)
    except HTTPError as e:
        body_text = e.read().decode("utf-8", errors="replace")[:200]
        raise RuntimeError(f"LLM API error ({e.code}): {body_text}")
    except URLError as e:
        raise RuntimeError(f"LLM API connection error: {e.reason}")


# ── Answer extraction & normalization ─────────────────

def _extract_verifier_answer(text: str) -> str:
    """Extract the answer from verifier response using multiple strategies."""
    text = text.strip()

    # Strategy 1: Look for explicit "ANSWER: ..." prefix (most reliable)
    answer_match = re.search(r'ANSWER:\s*(.+?)(?:\n|$)', text, re.IGNORECASE)
    if answer_match:
        return answer_match.group(1).strip()

    # Strategy 2: Look for "The answer is ..." or "Final answer: ..."
    for pattern in [
        r'(?:the\s+)?(?:final\s+)?answer\s+is[:\s]+(.+?)(?:\.|$)',
        r'(?:final\s+)?answer[:\s]+(.+?)(?:\.|$)',
    ]:
        m = re.search(pattern, text, re.IGNORECASE)
        if m:
            return m.group(1).strip()

    # Strategy 3: Last non-empty line (fallback)
    lines = [l.strip() for l in text.split('\n') if l.strip()]
    if lines:
        last = lines[-1]
        # Strip common prefixes
        for prefix in ['so ', 'therefore ', 'thus ', 'hence ', '= ', 'answer: ', 'result: ']:
            if last.lower().startswith(prefix):
                last = last[len(prefix):].strip()
        return last

    return text


def _normalize_for_compare(answer: str) -> str:
    """Aggressively normalize answer for comparison."""
    s = answer.strip().lower()
    # Remove surrounding quotes
    if len(s) >= 2 and s[0] == s[-1] and s[0] in ('"', "'"):
        s = s[1:-1].strip()
    # Remove trailing punctuation
    s = s.rstrip('.!,;:')
    # Remove common prefixes that verifiers add
    for prefix in ['the answer is ', 'answer: ', 'result: ', 'it is ', "it's "]:
        if s.startswith(prefix):
            s = s[len(prefix):].strip()
    # Remove surrounding quotes again (after prefix strip)
    if len(s) >= 2 and s[0] == s[-1] and s[0] in ('"', "'"):
        s = s[1:-1].strip()
    # Collapse whitespace
    s = re.sub(r"\s+", " ", s)
    # Remove spaces between single characters (e.g. "A G I K N P R" → "agiknpr")
    if re.match(r'^[a-z]( [a-z])+$', s):
        s = s.replace(' ', '')
    return s


def _answers_match(expected: str, actual: str) -> bool:
    """Check if two answers match with aggressive normalization."""
    a = _normalize_for_compare(expected)
    b = _normalize_for_compare(actual)

    if a == b:
        return True

    # Try numeric comparison (handles "30" vs "30.0")
    try:
        return abs(float(a) - float(b)) < 0.001
    except (ValueError, TypeError):
        pass

    # Try without spaces at all
    if a.replace(' ', '') == b.replace(' ', ''):
        return True

    # Try without commas (sorting answers: "1, 2, 3" vs "1,2,3")
    if a.replace(', ', ',').replace(' ', '') == b.replace(', ', ',').replace(' ', ''):
        return True

    return False


# ── Pre-validation ────────────────────────────────────

def _pre_validate_challenge(prompt: str, answer: str) -> Optional[str]:
    """Quick sanity checks before wasting an API call on verification.
    Returns error string if invalid, None if OK."""

    # Answer should be short
    if len(answer.split()) > 4:
        return f"Answer too long ({len(answer.split())} words): {answer!r}"

    # Answer shouldn't be a sentence
    if answer.endswith('.') and len(answer) > 10:
        return f"Answer looks like a sentence: {answer!r}"

    # Prompt should include the "reply with only" instruction
    lower_prompt = prompt.lower()
    if 'reply with only' not in lower_prompt and 'respond with only' not in lower_prompt:
        return "Prompt missing 'reply with only' instruction"

    # Reject riddles/trick questions
    riddle_signals = ['what am i', 'what has', 'i am a', 'riddle', 'what can']
    if any(sig in lower_prompt for sig in riddle_signals):
        return "Detected riddle/trick question"

    return None


# ── Dynamic challenge generator ──────────────────────

def generate_dynamic_challenge(
    provider_name: str,
    api_key: str,
    model: Optional[str] = None,
    verify_model: Optional[str] = None,
    max_retries: int = 3,
    timeout: int = 15,
) -> Optional[Tuple[str, str]]:
    """
    Generate a challenge using an LLM, then verify it with a second LLM call.

    Returns (prompt, answer) if successful, None if all retries fail.
    """
    for attempt in range(max_retries):
        try:
            # Step 1: Generate challenge (high temperature for variety)
            raw = _call_llm(
                provider_name, api_key,
                [{"role": "user", "content": GENERATE_PROMPT}],
                model=model,
                temperature=1.0,
                timeout=timeout,
            )

            # Parse JSON from response — may have work/explanation before it
            raw = raw.strip()
            # Remove markdown code fences
            raw = re.sub(r"```(?:json)?\s*", "", raw)
            raw = raw.replace("```", "")
            # Find the JSON object (last {...} in the response)
            json_matches = list(re.finditer(r'\{[^{}]*\}', raw))
            if not json_matches:
                logger.warning(f"Dynamic gen attempt {attempt+1}: no JSON found in response")
                continue
            json_str = json_matches[-1].group(0)  # Use last match (after work)

            challenge_data = json.loads(json_str)
            prompt = challenge_data.get("prompt", "").strip()
            expected_answer = challenge_data.get("answer", "").strip()

            if not prompt or not expected_answer:
                logger.warning(f"Dynamic gen attempt {attempt+1}: empty prompt/answer")
                continue

            # Step 1.5: Pre-validate before spending another API call
            pre_error = _pre_validate_challenge(prompt, expected_answer)
            if pre_error:
                logger.warning(f"Dynamic gen attempt {attempt+1}: pre-validation failed: {pre_error}")
                continue

            # Step 2: Verify by solving with LLM (low temperature for determinism)
            verify_response = _call_llm(
                provider_name, api_key,
                [{"role": "user", "content": VERIFY_PROMPT.format(prompt=prompt)}],
                model=verify_model or model,
                temperature=0.0,
                timeout=timeout,
            )

            # Extract answer using multiple strategies
            verify_answer = _extract_verifier_answer(verify_response)

            # Compare with aggressive normalization
            if _answers_match(expected_answer, verify_answer):
                logger.info(
                    f"Dynamic challenge verified (attempt {attempt+1}): "
                    f"{prompt[:60]}... → {expected_answer}"
                )
                return prompt, _normalize_for_compare(expected_answer)
            else:
                logger.warning(
                    f"Dynamic gen attempt {attempt+1}: verification mismatch "
                    f"(expected={expected_answer!r}, got={verify_answer!r})"
                )
                continue

        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Dynamic gen attempt {attempt+1}: parse error: {e}")
            continue
        except RuntimeError as e:
            logger.error(f"Dynamic gen attempt {attempt+1}: API error: {e}")
            continue
        except Exception as e:
            logger.error(f"Dynamic gen attempt {attempt+1}: unexpected error: {e}")
            continue

    logger.warning("Dynamic challenge generation failed after all retries, falling back to static")
    return None
