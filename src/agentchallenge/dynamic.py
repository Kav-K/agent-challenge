"""
Dynamic LLM-generated challenges.

When dynamic mode is enabled, challenges are generated by an LLM and then
verified by a second LLM call before being served. This produces novel,
creative challenges that can't be enumerated or cached.

⚠️ Dynamic mode adds 2 API requests to your hot path per challenge generation:
  1. One call to generate the challenge
  2. One call to verify/solve it

Supports: OpenAI, Anthropic, Google Gemini.
"""

import json
import logging
import os
import re
import time
from typing import Optional, Tuple
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError

logger = logging.getLogger("agentchallenge.dynamic")

# ── Provider configurations ───────────────────────────

PROVIDERS = {
    "openai": {
        "url": "https://api.openai.com/v1/chat/completions",
        "env_key": "OPENAI_API_KEY",
        "default_model": "gpt-4o-mini",
        "auth_header": lambda key: ("Authorization", f"Bearer {key}"),
        "build_body": lambda model, messages: json.dumps({
            "model": model,
            "messages": messages,
            "temperature": 1.0,
            "max_tokens": 300,
        }),
        "extract": lambda resp: resp["choices"][0]["message"]["content"].strip(),
    },
    "anthropic": {
        "url": "https://api.anthropic.com/v1/messages",
        "env_key": "ANTHROPIC_API_KEY",
        "default_model": "claude-sonnet-4-20250514",
        "auth_header": lambda key: ("x-api-key", key),
        "extra_headers": {"anthropic-version": "2023-06-01"},
        "build_body": lambda model, messages: json.dumps({
            "model": model,
            "max_tokens": 300,
            "messages": messages,
        }),
        "extract": lambda resp: resp["content"][0]["text"].strip(),
    },
    "google": {
        "env_key": "GOOGLE_API_KEY",
        "default_model": "gemini-2.0-flash",
        "build_url": lambda model, key: f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={key}",
        "build_body": lambda model, messages: json.dumps({
            "contents": [{"role": m["role"] if m["role"] != "assistant" else "model", "parts": [{"text": m["content"]}]} for m in messages],
            "generationConfig": {"temperature": 1.0, "maxOutputTokens": 300},
        }),
        "extract": lambda resp: resp["candidates"][0]["content"]["parts"][0]["text"].strip(),
    },
}

# ── Generation prompt ────────────────────────────────

GENERATE_PROMPT = """Generate a unique reasoning challenge for an AI agent to solve.

Requirements:
- The challenge must be solvable by reasoning/logic alone (no web search, no tools, no code execution)
- There must be exactly ONE correct answer with no ambiguity
- The answer should be short (a word, number, or short phrase)
- Be creative — use wordplay, logic puzzles, ciphers, pattern recognition, math, etc.
- Vary the difficulty: some easy, some tricky
- DO NOT reuse common examples — generate something novel each time

Output format (strict JSON, no markdown):
{"prompt": "the challenge text for the agent", "answer": "the correct answer"}

Example outputs (do NOT reuse these):
{"prompt": "If you remove the first and last letter of STRANGE, what word remains?", "answer": "trang"}
{"prompt": "What is the 7th prime number?", "answer": "17"}
{"prompt": "Replace each vowel in ROBOT with the next vowel (A→E, E→I, I→O, O→U, U→A). What do you get?", "answer": "RUBUT"}

Generate one challenge now:"""

VERIFY_PROMPT_TEMPLATE = """Solve this challenge. Think step by step, then give ONLY the final answer on the last line.

Challenge: {prompt}

Your final answer (just the answer, nothing else):"""


# ── LLM caller ───────────────────────────────────────

def _call_llm(provider_name: str, api_key: str, messages: list, model: Optional[str] = None, timeout: int = 15) -> str:
    """Make a raw HTTP call to an LLM provider. Returns the text response."""
    provider = PROVIDERS[provider_name]
    model = model or provider["default_model"]

    # Build URL
    if "build_url" in provider:
        url = provider["build_url"](model, api_key)
    else:
        url = provider["url"]

    # Build request
    body = provider["build_body"](model, messages).encode("utf-8")
    req = Request(url, data=body, method="POST")
    req.add_header("Content-Type", "application/json")

    # Auth
    if "auth_header" in provider:
        header_name, header_val = provider["auth_header"](api_key)
        req.add_header(header_name, header_val)

    # Extra headers
    for k, v in provider.get("extra_headers", {}).items():
        req.add_header(k, v)

    # Call
    try:
        with urlopen(req, timeout=timeout) as resp:
            data = json.loads(resp.read().decode("utf-8"))
            return provider["extract"](data)
    except HTTPError as e:
        body_text = e.read().decode("utf-8", errors="replace")[:200]
        raise RuntimeError(f"LLM API error ({e.code}): {body_text}")
    except URLError as e:
        raise RuntimeError(f"LLM API connection error: {e.reason}")


# ── Dynamic challenge generator ──────────────────────

def generate_dynamic_challenge(
    provider_name: str,
    api_key: str,
    model: Optional[str] = None,
    verify_model: Optional[str] = None,
    max_retries: int = 3,
    timeout: int = 15,
) -> Optional[Tuple[str, str]]:
    """
    Generate a challenge using an LLM, then verify it with a second LLM call.

    Returns (prompt, answer) if successful, None if all retries fail.
    """
    for attempt in range(max_retries):
        try:
            # Step 1: Generate challenge
            raw = _call_llm(
                provider_name, api_key,
                [{"role": "user", "content": GENERATE_PROMPT}],
                model=model, timeout=timeout,
            )

            # Parse JSON response (handle markdown code fences)
            raw = raw.strip()
            if raw.startswith("```"):
                raw = re.sub(r"^```(?:json)?\s*", "", raw)
                raw = re.sub(r"\s*```$", "", raw)

            challenge_data = json.loads(raw)
            prompt = challenge_data.get("prompt", "").strip()
            expected_answer = challenge_data.get("answer", "").strip()

            if not prompt or not expected_answer:
                logger.warning(f"Dynamic gen attempt {attempt+1}: empty prompt/answer")
                continue

            # Step 2: Verify by solving with LLM
            verify_response = _call_llm(
                provider_name, api_key,
                [{"role": "user", "content": VERIFY_PROMPT_TEMPLATE.format(prompt=prompt)}],
                model=verify_model or model, timeout=timeout,
            )

            # Extract just the last line as the answer
            verify_answer = verify_response.strip().split("\n")[-1].strip()

            # Normalize both for comparison
            norm_expected = _normalize_for_compare(expected_answer)
            norm_verify = _normalize_for_compare(verify_answer)

            if norm_expected == norm_verify:
                logger.info(f"Dynamic challenge verified (attempt {attempt+1}): {prompt[:60]}...")
                return prompt, expected_answer.lower()
            else:
                logger.warning(
                    f"Dynamic gen attempt {attempt+1}: verification mismatch "
                    f"(expected={expected_answer!r}, got={verify_answer!r})"
                )
                continue

        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Dynamic gen attempt {attempt+1}: parse error: {e}")
            continue
        except RuntimeError as e:
            logger.error(f"Dynamic gen attempt {attempt+1}: API error: {e}")
            continue
        except Exception as e:
            logger.error(f"Dynamic gen attempt {attempt+1}: unexpected error: {e}")
            continue

    logger.warning("Dynamic challenge generation failed after all retries, falling back to static")
    return None


def _normalize_for_compare(answer: str) -> str:
    """Normalize answer for verification comparison — more lenient than token comparison."""
    s = answer.strip().lower()
    # Remove surrounding quotes
    if len(s) >= 2 and s[0] == s[-1] and s[0] in ('"', "'"):
        s = s[1:-1].strip()
    # Remove trailing period
    s = s.rstrip(".")
    # Collapse whitespace
    s = re.sub(r"\s+", " ", s)
    return s
